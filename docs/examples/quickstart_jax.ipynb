{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da9fd6a8",
   "metadata": {},
   "source": [
    "## Getting Started with JAX + Flax\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Summary</b>\n",
    "    \n",
    "We build a basic Transformer layer using regular JAX and Flax modules. This will be our baseline \n",
    "for later comparisons with Transformer Engine.\n",
    "\n",
    "</div>\n",
    "\n",
    "We will construct the components as follows:\n",
    "\n",
    "- `LayerNorm`: `flax.linen.LayerNorm`\n",
    "- `QKV Projection`: `flax.linen.DenseGeneral` (conceptually three `Dense` layers for Q, K, and V \n",
    "  separately, but we fuse into a single `Linear` layer that is three times larger)\n",
    "- `DotProductAttention`: `DotProductAttention` from \n",
    "  [quickstart_jax_utils.py](quickstart_jax_utils.py)\n",
    "- `Projection`: `flax.linen.DenseGeneral`\n",
    "- `Dropout`: `flax.linen.Dropout`\n",
    "- `MLP`: `BasicMLP` from [quickstart_jax_utils.py](quickstart_jax_utils.py)\n",
    "\n",
    "Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2be43d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import quickstart_jax_utils as utils\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "class BasicTransformerLayer(nn.Module):\n",
    "    hidden_size: int\n",
    "    ffn_hidden_size: int\n",
    "    num_attention_heads: int\n",
    "    layernorm_eps: Optional[float] = 1e-5\n",
    "    attention_dropout: Optional[float] = 0.1\n",
    "    hidden_dropout: Optional[float] = 0.1\n",
    "    dtype: Optional[type] = jnp.float16\n",
    "    \n",
    "    def setup(self):\n",
    "        # Safeguard layer-norm epsilon with machine epsilon\n",
    "        typed_perturb = self.dtype(self.layernorm_eps)\n",
    "        typed_macheps = self.dtype(jnp.finfo(self.dtype).eps)\n",
    "        epsilon = jax.lax.max(typed_perturb, typed_macheps)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(float(epsilon), dtype=self.dtype)\n",
    "        self.qkv_projection = nn.DenseGeneral(3 * self.hidden_size, dtype=self.dtype)\n",
    "        self.kv_channels = self.hidden_size // self.num_attention_heads\n",
    "        self.attention = utils.DotProductAttention(\n",
    "            num_attention_heads=self.num_attention_heads,\n",
    "            kv_channels=self.kv_channels,\n",
    "            dropout_rate=self.attention_dropout,\n",
    "            dropout_rng='attention',\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.projection = nn.DenseGeneral(self.hidden_size, dtype=self.dtype)\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout, rng_collection='hidden')\n",
    "        self.ln2 = nn.LayerNorm(float(epsilon), dtype=self.dtype)\n",
    "        self.mlp = utils.BasicMLP(\n",
    "            hidden_size=self.hidden_size,\n",
    "            ffn_hidden_size=self.ffn_hidden_size,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(\n",
    "        self, \n",
    "        x: jnp.ndarray,\n",
    "        attention_mask: jnp.ndarray,\n",
    "        train: Optional[bool] = False\n",
    "    ) -> jnp.ndarray:\n",
    "        # Multi-Head Attention from Vaswani et al. \"Attention is All You Need\":\n",
    "        #   Layer-norm -> QKV proj. -> DotProductAttention -> context proj. -> dropout -> residual\n",
    "        res = x\n",
    "        x = self.ln1(x)\n",
    "        qkv = self.qkv_projection(x)\n",
    "        qkv = jnp.reshape(\n",
    "            qkv,\n",
    "            (qkv.shape[0], qkv.shape[1], self.num_attention_heads, 3 * self.kv_channels)\n",
    "        )\n",
    "        q, k, v = jnp.split(qkv, 3, axis=3)\n",
    "        x = self.attention(q, k, v, attention_mask=attention_mask, train=train)\n",
    "        x = self.projection(x)\n",
    "        x = self.dropout(x, deterministic=(not train))\n",
    "        x = res + x\n",
    "\n",
    "        # Output layernorm -> multi-layer perceptron -> residual\n",
    "        res = x\n",
    "        x = self.ln2(x)\n",
    "        x = self.mlp(x)\n",
    "        return x + res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40724d1d",
   "metadata": {},
   "source": [
    "That's it! We now have a simple Transformer layer. Before testing it, we first have to set problem sizes, create the necessary random number generators, and initialize the layer parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a786f0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicTransformerLayer(\n",
      "    # attributes\n",
      "    hidden_size = 1024\n",
      "    ffn_hidden_size = 4096\n",
      "    num_attention_heads = 16\n",
      "    layernorm_eps = 1e-05\n",
      "    attention_dropout = 0.1\n",
      "    hidden_dropout = 0.1\n",
      "    dtype = float16\n",
      ")\n",
      "\n",
      "params\n",
      "|__ln1\n",
      "|  |__scale: (1024,)\n",
      "|  |__bias: (1024,)\n",
      "|__qkv_projection\n",
      "|  |__kernel: (1024, 3072)\n",
      "|  |__bias: (3072,)\n",
      "|__projection\n",
      "|  |__kernel: (1024, 1024)\n",
      "|  |__bias: (1024,)\n",
      "|__ln2\n",
      "|  |__scale: (1024,)\n",
      "|  |__bias: (1024,)\n",
      "|__mlp\n",
      "   |__linear1\n",
      "   |  |__kernel: (1024, 4096)\n",
      "   |  |__bias: (4096,)\n",
      "   |__linear2\n",
      "      |__kernel: (4096, 1024)\n",
      "      |__bias: (1024,)\n"
     ]
    }
   ],
   "source": [
    "# Layer configuration:\n",
    "batch_size = 32\n",
    "sequence_length = 128\n",
    "num_attention_heads = 16\n",
    "head_size = 64\n",
    "hidden_size = num_attention_heads * head_size\n",
    "ffn_hidden_size = 4 * hidden_size\n",
    "dtype = jnp.float16\n",
    "\n",
    "# Create the necessary RNG keys:\n",
    "#   The dropout in DotProductAttention will use the 'attention' key group, while\n",
    "#   the hidden layer dropout in BasicTransformerLayer will use 'hidden' key group.\n",
    "root_key = jax.random.PRNGKey(seed=0)\n",
    "fwd_key, bwd_key, params_key, attention_key, hidden_key = jax.random.split(root_key, 5)\n",
    "rngs = {\n",
    "    'params' : params_key,\n",
    "    'attention' : attention_key,\n",
    "    'hidden' : hidden_key\n",
    "}\n",
    "\n",
    "# Synthetic data:\n",
    "x = jax.device_put(\n",
    "    jax.random.uniform(fwd_key, (sequence_length, batch_size, hidden_size), dtype=dtype)\n",
    ")\n",
    "\n",
    "# Initialize the module and inspect parameters:\n",
    "basic_transformer = BasicTransformerLayer(\n",
    "    hidden_size,\n",
    "    ffn_hidden_size,\n",
    "    num_attention_heads,\n",
    "    dtype=dtype\n",
    ")\n",
    "print(basic_transformer, end='\\n\\n')\n",
    "flax_params = basic_transformer.init(rngs, jnp.zeros_like(x), attention_mask=None, train=True)\n",
    "utils.inspect_params(flax_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test our implementation with a sum squared loss over the Transformer layer output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Execution time:\n",
      "84.4 ms ± 1.03 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Mean squared error loss function:\n",
    "def sum_squared_loss(module, params, x, **kwargs):\n",
    "    out = module.apply(params, x, **kwargs)\n",
    "    out = jnp.reshape(out, (out.size,))\n",
    "    return jnp.dot(out, out).astype(out.dtype)\n",
    "\n",
    "# Autograd sum_squared_loss for BasicTransformer w.r.t. input X:\n",
    "fwd_bwd_func_flax = jax.value_and_grad(partial(sum_squared_loss, basic_transformer), argnums=(0, 1))\n",
    "\n",
    "# Named arguments for the loss function:\n",
    "fwd_kwargs = {\n",
    "    'attention_mask': None,\n",
    "    'train' : True,\n",
    "    'rngs' : rngs\n",
    "}\n",
    "\n",
    "# Warmup iterations:\n",
    "for _ in range(50):\n",
    "    y, (dp, dy) = fwd_bwd_func_flax(flax_params, x, **fwd_kwargs)\n",
    "\n",
    "# Timed iterations:\n",
    "print(\"\\nExecution time:\")\n",
    "%timeit -r 5 -n 10 jax.block_until_ready(fwd_bwd_func_flax(flax_params, x, **fwd_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43717e36",
   "metadata": {},
   "source": [
    "## Meet Transformer Engine\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Summary</b>\n",
    "    \n",
    "We modify the example Transformer layer to include the simplest TE modules: `Linear` and `LayerNorm`.\n",
    "\n",
    "</div>\n",
    "\n",
    "Now that we have a basic Transformer layer, let's use Transformer Engine to speed up the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_engine.jax as te"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1931f911",
   "metadata": {},
   "source": [
    "TE provides a set of Flax modules that can be used to build Transformer layers. The simplest of the provided modules are the `Linear` and `LayerNorm` layers, which we can use instead of `flax.linen.Linear` and `flax.linen.LayerNorm`. Let's modify `BasicTransformerLayer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f44db50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicTEMLP(nn.Module):\n",
    "    hidden_size: int\n",
    "    ffn_hidden_size: int\n",
    "    dtype : Optional[type] = jnp.float16\n",
    "\n",
    "    def setup(self):\n",
    "        self.linear1 = te.flax.DenseGeneral(self.ffn_hidden_size, dtype=self.dtype)\n",
    "        self.linear2 = te.flax.DenseGeneral(self.hidden_size, dtype=self.dtype)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x: jnp.ndarray) -> jnp.ndarray:\n",
    "        x = self.linear1(x)\n",
    "        x = jax.nn.gelu(x, approximate=True)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasicTETransformerLayer(nn.Module):\n",
    "    hidden_size: int\n",
    "    ffn_hidden_size: int\n",
    "    num_attention_heads: int\n",
    "    layernorm_eps: Optional[float] = 1e-5\n",
    "    attention_dropout: Optional[float] = 0.1\n",
    "    hidden_dropout: Optional[float] = 0.1\n",
    "    dtype: Optional[type] = jnp.float16\n",
    "\n",
    "    def setup(self):\n",
    "        self.ln1 = te.flax.LayerNorm(\n",
    "            epsilon=self.layernorm_eps,\n",
    "            dtype=self.dtype,\n",
    "            transpose_batch_sequence=True\n",
    "        )\n",
    "        self.qkv_projection = te.flax.DenseGeneral(\n",
    "            3 * self.hidden_size, dtype=self.dtype\n",
    "        )\n",
    "        self.kv_channels = self.hidden_size // self.num_attention_heads\n",
    "        self.attention = utils.DotProductAttention(\n",
    "            num_attention_heads=self.num_attention_heads,\n",
    "            kv_channels=self.kv_channels,\n",
    "            dropout_rate=self.attention_dropout,\n",
    "            dropout_rng='attention',\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.projection = te.flax.DenseGeneral(hidden_size, dtype=self.dtype)\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout, rng_collection='hidden')\n",
    "        self.ln2 = te.flax.LayerNorm(\n",
    "            epsilon=self.layernorm_eps,\n",
    "            dtype=self.dtype,\n",
    "            transpose_batch_sequence=True\n",
    "        )\n",
    "        self.mlp = BasicTEMLP(\n",
    "            hidden_size=self.hidden_size,\n",
    "            ffn_hidden_size=self.ffn_hidden_size,\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                x: jnp.ndarray,\n",
    "                attention_mask: jnp.ndarray,\n",
    "                train: Optional[bool] = False\n",
    "    ) -> jnp.ndarray:\n",
    "        res = x\n",
    "        x = self.ln1(x)\n",
    "        qkv = self.qkv_projection(x)\n",
    "        qkv = jnp.reshape(\n",
    "            qkv, (qkv.shape[0], qkv.shape[1], self.num_attention_heads, 3 * self.kv_channels)\n",
    "        )\n",
    "        q, k, v = jnp.split(qkv, 3, axis=3)\n",
    "        x = self.attention(q, k, v, attention_mask=attention_mask)\n",
    "        x = self.projection(x)\n",
    "        x = self.dropout(x, deterministic=(not train))\n",
    "        x = res + x\n",
    "        \n",
    "        res = x\n",
    "        x = self.ln2(x.astype(self.dtype))\n",
    "        x = self.mlp(x)\n",
    "        return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "916531e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicTETransformerLayer(\n",
      "    # attributes\n",
      "    hidden_size = 1024\n",
      "    ffn_hidden_size = 4096\n",
      "    num_attention_heads = 16\n",
      "    layernorm_eps = 1e-05\n",
      "    attention_dropout = 0.1\n",
      "    hidden_dropout = 0.1\n",
      "    dtype = float16\n",
      ")\n",
      "\n",
      "params\n",
      "|__ln1\n",
      "|  |__scale: (1024,)\n",
      "|  |__ln_bias: (1024,)\n",
      "|__qkv_projection\n",
      "|  |__kernel: (1024, 3072)\n",
      "|  |__bias: (3072,)\n",
      "|__projection\n",
      "|  |__kernel: (1024, 1024)\n",
      "|  |__bias: (1024,)\n",
      "|__ln2\n",
      "|  |__scale: (1024,)\n",
      "|  |__ln_bias: (1024,)\n",
      "|__mlp\n",
      "   |__linear1\n",
      "   |  |__kernel: (1024, 4096)\n",
      "   |  |__bias: (4096,)\n",
      "   |__linear2\n",
      "      |__kernel: (4096, 1024)\n",
      "      |__bias: (1024,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the module and inspect parameters:\n",
    "basic_te_transformer = BasicTETransformerLayer(\n",
    "    hidden_size,\n",
    "    ffn_hidden_size,\n",
    "    num_attention_heads,\n",
    "    dtype=dtype\n",
    ")\n",
    "print(basic_te_transformer, end='\\n\\n')\n",
    "\n",
    "# Share parameters with the Flax implementation\n",
    "te_params = basic_te_transformer.init(rngs, jnp.zeros_like(x), attention_mask=None, train=True)\n",
    "te_params = utils.share_params(te_params, flax_params)\n",
    "utils.inspect_params(te_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.1 ms ± 1.08 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "fwd_bwd_func_te = jax.value_and_grad(partial(sum_squared_loss, basic_te_transformer), argnums=(0, 1))\n",
    "\n",
    "# Warmup iterations:\n",
    "for _ in range(50):\n",
    "    y, (dp, dy) = fwd_bwd_func_te(te_params, x, **fwd_kwargs)\n",
    "\n",
    "# Timed iterations:\n",
    "%timeit -r 5 -n 10 jax.block_until_ready(fwd_bwd_func_te(te_params, x, **fwd_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f990226",
   "metadata": {},
   "source": [
    "## Fused TE Modules\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Summary</b>\n",
    "    \n",
    "We optimize the example Transformer layer with TE modules for fused operations.\n",
    "\n",
    "</div>\n",
    "\n",
    "The `Linear` layer is enough to build any Transformer model and it enables usage of Transformer Engine even for very custom Transformers. However, having more knowledge about the model allows for additional optimizations like kernel fusion, increasing the achievable speedup.\n",
    "\n",
    "Transformer Engine therefore provides coarser modules that span multiple layers:\n",
    "\n",
    "* `LayerNormDenseGeneral`\n",
    "* `LayerNormMLP`\n",
    "* `TransformerLayer`\n",
    "\n",
    "Building a third iteration of our Transformer layer with `LayerNormDenseGeneral` and `LayerNormMLP`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c55eae1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedTETransformerLayer(nn.Module):\n",
    "    hidden_size: int\n",
    "    ffn_hidden_size: int\n",
    "    num_attention_heads: int\n",
    "    layernorm_eps: Optional[float] = 1e-5\n",
    "    attention_dropout: Optional[float] = 0.1\n",
    "    hidden_dropout: Optional[float] = 0.1\n",
    "    dtype: Optional[type] = jnp.float16\n",
    "\n",
    "    def setup(self):\n",
    "        self.ln_qkv = te.flax.LayerNormDenseGeneral(\n",
    "            3 * self.hidden_size,\n",
    "            epsilon=self.layernorm_eps,\n",
    "            return_layernorm_output=False,\n",
    "            dtype=self.dtype,\n",
    "            transpose_batch_sequence=True\n",
    "        )\n",
    "        self.kv_channels = self.hidden_size // self.num_attention_heads\n",
    "        self.attention = utils.DotProductAttention(\n",
    "            num_attention_heads=self.num_attention_heads,\n",
    "            kv_channels=self.kv_channels,\n",
    "            dropout_rate=self.attention_dropout,\n",
    "            dropout_rng='attention',\n",
    "            dtype=self.dtype\n",
    "        )\n",
    "        self.projection = te.flax.DenseGeneral(hidden_size, dtype=self.dtype)\n",
    "        self.dropout = nn.Dropout(self.hidden_dropout, rng_collection='hidden')\n",
    "        self.ln_mlp = te.flax.LayerNormMLP(\n",
    "            self.ffn_hidden_size,\n",
    "            epsilon=self.layernorm_eps,\n",
    "            return_layernorm_output=False,\n",
    "            activations=('gelu','linear'),\n",
    "            intermediate_dropout_rate=0.0,\n",
    "            dtype=self.dtype,\n",
    "            transpose_batch_sequence=True\n",
    "        )\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "                x: jnp.ndarray,\n",
    "                attention_mask: jnp.ndarray,\n",
    "                train: Optional[bool] = False\n",
    "    ) -> jnp.ndarray:\n",
    "        res = x\n",
    "        qkv, _ = self.ln_qkv(x)\n",
    "        qkv = jnp.reshape(\n",
    "            qkv, (qkv.shape[0], qkv.shape[1], self.num_attention_heads, 3 * self.kv_channels)\n",
    "        )\n",
    "        q, k, v = jnp.split(qkv, 3, axis=3)\n",
    "        x = self.attention(q, k, v, attention_mask=attention_mask)\n",
    "        x = self.projection(x)\n",
    "        x = self.dropout(x, deterministic=(not train))\n",
    "        x = res + x\n",
    "        \n",
    "        res = x\n",
    "        x, _ = self.ln_mlp(x.astype(self.dtype))\n",
    "        return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85949421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FusedTETransformerLayer(\n",
      "    # attributes\n",
      "    hidden_size = 1024\n",
      "    ffn_hidden_size = 4096\n",
      "    num_attention_heads = 16\n",
      "    layernorm_eps = 1e-05\n",
      "    attention_dropout = 0.1\n",
      "    hidden_dropout = 0.1\n",
      "    dtype = float16\n",
      ")\n",
      "\n",
      "params\n",
      "|__ln_qkv\n",
      "|  |__scale: (1024,)\n",
      "|  |__ln_bias: (1024,)\n",
      "|  |__kernel: (1024, 3072)\n",
      "|__projection\n",
      "|  |__kernel: (1024, 1024)\n",
      "|  |__bias: (1024,)\n",
      "|__ln_mlp\n",
      "   |__scale: (1024,)\n",
      "   |__ln_bias: (1024,)\n",
      "   |__wi_kernel: (1024, 2, 4096)\n",
      "   |__wo_kernel: (4096, 1024)\n"
     ]
    }
   ],
   "source": [
    "fused_te_transformer = FusedTETransformerLayer(\n",
    "    hidden_size,\n",
    "    ffn_hidden_size,\n",
    "    num_attention_heads,\n",
    "    dtype=dtype\n",
    ")\n",
    "print(fused_te_transformer, end='\\n\\n')\n",
    "\n",
    "fused_te_params = fused_te_transformer.init(rngs, jnp.zeros_like(x), attention_mask=None, train=True)\n",
    "fused_te_params = utils.share_params(fused_te_params, flax_params)\n",
    "utils.inspect_params(fused_te_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24e101bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.5 ms ± 1.22 ms per loop (mean ± std. dev. of 5 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "fwd_bwd_func_fused_te = jax.value_and_grad(partial(sum_squared_loss, fused_te_transformer), argnums=(0, 1))\n",
    "\n",
    "# Warmup iterations:\n",
    "for _ in range(50):\n",
    "    y, (dp, dy) = fwd_bwd_func_fused_te(fused_te_params, x, **fwd_kwargs)\n",
    "\n",
    "# Timed iterations:\n",
    "%timeit -r 5 -n 10 jax.block_until_ready(fwd_bwd_func_fused_te(fused_te_params, x, **fwd_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f13c26",
   "metadata": {},
   "source": [
    "Finally, the `TransformerLayer` module is convenient for creating standard Transformer architectures and it provides the highest degree of performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerLayer(\n",
      "    # attributes\n",
      "    hidden_size = 1024\n",
      "    mlp_hidden_size = 4096\n",
      "    num_attention_heads = 16\n",
      "    layernorm_type = 'layernorm'\n",
      "    layernorm_epsilon = 1e-05\n",
      "    zero_centered_gamma = False\n",
      "    hidden_dropout = 0.1\n",
      "    hidden_dropout_dims = ()\n",
      "    attention_dropout = 0.1\n",
      "    intermediate_dropout = 0.0\n",
      "    intermediate_dropout_dims = ()\n",
      "    dropout_rng_name = 'transformer'\n",
      "    mha_kernel_init = init\n",
      "    mlp_kernel_init = init\n",
      "    mlp_activations = ('gelu', 'linear')\n",
      "    use_bias = False\n",
      "    bias_init = zeros\n",
      "    apply_residual_connection_post_layernorm = False\n",
      "    output_layernorm = False\n",
      "    float32_attention_logits = False\n",
      "    layer_type = <TransformerLayerType.ENCODER: 'encoder'>\n",
      "    self_attn_mask_type = 'causal'\n",
      "    enable_relative_embedding = False\n",
      "    relative_embedding = None\n",
      "    dtype = float16\n",
      "    drop_path = 0.0\n",
      "    fuse_qkv_params = True\n",
      "    transpose_batch_sequence = True\n",
      "    scale_attn_logits = False\n",
      "    scaled_query_init = True\n",
      ")\n",
      "\n",
      "params\n",
      "|__attention\n",
      "|  |__qkv\n",
      "|  |  |__scale: (1024,)\n",
      "|  |  |__ln_bias: (1024,)\n",
      "|  |  |__kernel: (1024, 3, 1024)\n",
      "|  |__out\n",
      "|     |__kernel: (1024, 1024)\n",
      "|__mlp\n",
      "   |__scale: (1024,)\n",
      "   |__ln_bias: (1024,)\n",
      "   |__wi_kernel: (1024, 2, 4096)\n",
      "   |__wo_kernel: (4096, 1024)\n"
     ]
    }
   ],
   "source": [
    "native_te_transformer = te.flax.TransformerLayer(\n",
    "    hidden_size,\n",
    "    ffn_hidden_size,\n",
    "    num_attention_heads,\n",
    "    layernorm_epsilon=1e-5,\n",
    "    intermediate_dropout=0.0,\n",
    "    dropout_rng_name='transformer',\n",
    "    mlp_activations=('gelu','linear'),\n",
    "    enable_relative_embedding=False,\n",
    "    dtype=dtype,\n",
    "    transpose_batch_sequence=True\n",
    ")\n",
    "print(native_te_transformer, end='\\n\\n')\n",
    "\n",
    "native_te_rngs = {\n",
    "    'params' : params_key,\n",
    "    'transformer' : jax.random.PRNGKey(seed=1)\n",
    "}\n",
    "native_te_params = native_te_transformer.init(\n",
    "    native_te_rngs,\n",
    "    jnp.zeros_like(x),\n",
    "    attention_mask=None,\n",
    "    deterministic=False   # train=True\n",
    ")\n",
    "native_te_params = utils.share_params(native_te_params, flax_params)\n",
    "utils.inspect_params(native_te_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.4 ms ± 955 µs per loop (mean ± std. dev. of 5 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "fwd_bwd_func_native_te = jax.value_and_grad(partial(sum_squared_loss, native_te_transformer), argnums=(0, 1))\n",
    "\n",
    "native_te_kwargs = {\n",
    "    'attention_mask' : None,\n",
    "    'deterministic' : False,\n",
    "    'rngs' : native_te_rngs\n",
    "}\n",
    "\n",
    "# Warmup iterations:\n",
    "for _ in range(50):\n",
    "    y, (dp, dy) = fwd_bwd_func_native_te(native_te_params, x, **native_te_kwargs)\n",
    "\n",
    "# Timed iterations:\n",
    "%timeit -r 5 -n 10 jax.block_until_ready(fwd_bwd_func_native_te(native_te_params, x, **native_te_kwargs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4034c3eb-8958-49f2-85f6-30c94977d884",
   "metadata": {},
   "source": [
    "## Enabling FP8\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Summary</b>\n",
    "    \n",
    "We configure a TE module to perform compute in FP8.\n",
    "\n",
    "</div>\n",
    "\n",
    "Enabling FP8 support is very simple in Transformer Engine. We just need to wrap the modules within an [fp8_autocast](../api/pytorch.rst#transformer_engine.pytorch.fp8_autocast) context manager. Note that fp8_autocast should only be used to wrap the forward pass and must exit before starting a backward pass. See the [FP8 tutorial](fp8_primer.ipynb) for a detailed explanation of FP8 recipes and the supported options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31256aa7-3d5e-425c-91ab-502b1326a748",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Device compute capability 8.9 or higher required for FP8 execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/devroot/code/te-jax-dev/TransformerEngine/docs/examples/quickstart_jax.ipynb Cell 21\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f666c65786f2d74652d6a61782d6465762d72756e2d373538663432646532386361227d@ssh-remote%2Bflexo.local/devroot/code/te-jax-dev/TransformerEngine/docs/examples/quickstart_jax.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformer_engine\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcommon\u001b[39;00m \u001b[39mimport\u001b[39;00m recipe\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f666c65786f2d74652d6a61782d6465762d72756e2d373538663432646532386361227d@ssh-remote%2Bflexo.local/devroot/code/te-jax-dev/TransformerEngine/docs/examples/quickstart_jax.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m fp8_recipe \u001b[39m=\u001b[39m recipe\u001b[39m.\u001b[39mDelayedScaling(margin\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, interval\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, fp8_format\u001b[39m=\u001b[39mrecipe\u001b[39m.\u001b[39mFormat\u001b[39m.\u001b[39mHYBRID)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f666c65786f2d74652d6a61782d6465762d72756e2d373538663432646532386361227d@ssh-remote%2Bflexo.local/devroot/code/te-jax-dev/TransformerEngine/docs/examples/quickstart_jax.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m te\u001b[39m.\u001b[39mfp8_autocast(enabled\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, fp8_recipe\u001b[39m=\u001b[39mfp8_recipe):\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f666c65786f2d74652d6a61782d6465762d72756e2d373538663432646532386361227d@ssh-remote%2Bflexo.local/devroot/code/te-jax-dev/TransformerEngine/docs/examples/quickstart_jax.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     fp8_te_transformer \u001b[39m=\u001b[39m te\u001b[39m.\u001b[39mflax\u001b[39m.\u001b[39mTransformerLayer(\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f666c65786f2d74652d6a61782d6465762d72756e2d373538663432646532386361227d@ssh-remote%2Bflexo.local/devroot/code/te-jax-dev/TransformerEngine/docs/examples/quickstart_jax.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         hidden_size,\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f666c65786f2d74652d6a61782d6465762d72756e2d373538663432646532386361227d@ssh-remote%2Bflexo.local/devroot/code/te-jax-dev/TransformerEngine/docs/examples/quickstart_jax.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         ffn_hidden_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f666c65786f2d74652d6a61782d6465762d72756e2d373538663432646532386361227d@ssh-remote%2Bflexo.local/devroot/code/te-jax-dev/TransformerEngine/docs/examples/quickstart_jax.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m         transpose_batch_sequence\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f666c65786f2d74652d6a61782d6465762d72756e2d373538663432646532386361227d@ssh-remote%2Bflexo.local/devroot/code/te-jax-dev/TransformerEngine/docs/examples/quickstart_jax.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f666c65786f2d74652d6a61782d6465762d72756e2d373538663432646532386361227d@ssh-remote%2Bflexo.local/devroot/code/te-jax-dev/TransformerEngine/docs/examples/quickstart_jax.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     fp8_te_params \u001b[39m=\u001b[39m fp8_te_transformer\u001b[39m.\u001b[39minit(native_te_rngs, jnp\u001b[39m.\u001b[39mzeros_like(x), attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/devroot/code/te-jax-dev/TransformerEngine/transformer_engine/jax/fp8.py:397\u001b[0m, in \u001b[0;36mfp8_autocast\u001b[0;34m(enabled, fp8_recipe, mesh_resource)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m enabled:\n\u001b[1;32m    396\u001b[0m     fp8_available, reason_for_no_fp8 \u001b[39m=\u001b[39m is_fp8_available()\n\u001b[0;32m--> 397\u001b[0m     \u001b[39massert\u001b[39;00m fp8_available, reason_for_no_fp8\n\u001b[1;32m    399\u001b[0m     amax_compute_algo \u001b[39m=\u001b[39m AmaxComputeAlgo\u001b[39m.\u001b[39mMOST_RECENT\n\u001b[1;32m    400\u001b[0m     \u001b[39mif\u001b[39;00m fp8_recipe\u001b[39m.\u001b[39mamax_compute_algo \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmax\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Device compute capability 8.9 or higher required for FP8 execution."
     ]
    }
   ],
   "source": [
    "from transformer_engine.common import recipe\n",
    "\n",
    "fp8_recipe = recipe.DelayedScaling(margin=0, interval=1, fp8_format=recipe.Format.HYBRID)\n",
    "with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe):\n",
    "    fp8_te_transformer = te.flax.TransformerLayer(\n",
    "        hidden_size,\n",
    "        ffn_hidden_size,\n",
    "        num_attention_heads,\n",
    "        layernorm_epsilon=1e-5,\n",
    "        intermediate_dropout=0.0,\n",
    "        dropout_rng_name='transformer',\n",
    "        mlp_activations=('gelu','linear'),\n",
    "        enable_relative_embedding=False,\n",
    "        dtype=dtype,\n",
    "        transpose_batch_sequence=True\n",
    "    )\n",
    "\n",
    "    fp8_te_params = fp8_te_transformer.init(native_te_rngs, jnp.zeros_like(x), attention_mask=None)\n",
    "    fwd_bwd_func_fp8_te = jax.value_and_grad(partial(sum_squared_loss, fp8_te_transformer), argnums=(0, 1))\n",
    "\n",
    "    # Warmup iterations:\n",
    "    for _ in range(50):\n",
    "        y, (dp, dy) = fwd_bwd_func_fp8_te(fp8_te_arams, x, **native_te_kwargs)\n",
    "\n",
    "    # Timed iterations:\n",
    "    %timeit -r 5 -n 10 jax.block_until_ready(fwd_bwd_func_fp8_te(fp8_te_params, x, **native_te_kwargs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU training\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<b>Summary</b>\n",
    "    \n",
    "We parallelize a Transformer layer with data, tensor, and sequence parallelism. We also demonstrate how a Transformer layer can be wrapped with PyTorch's Fully Sharded Data Parallel strategy.\n",
    "\n",
    "</div>\n",
    "\n",
    "A variety of parallelism strategies can be used to enable multi-GPU training of Transformer models, often based on different approaches to distribute their $\\text{sequence_length} \\times \\text{batch_size} \\times \\text{hidden_size}$ activation tensors. In this section, we will build on the GPT Transformer layer from the [quickstart guide](quickstart.ipynb) to demonstrate each of these parallelism strategies. As a refresher, we share below the same layer configuration and synthetic data we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_engine.common.recipe import Format, DelayedScaling\n",
    "\n",
    "# Layer configuration\n",
    "hidden_size = 4096\n",
    "sequence_length = 2048\n",
    "batch_size = 4\n",
    "ffn_hidden_size = 16384\n",
    "num_attention_heads = 32\n",
    "dtype = torch.float16\n",
    "\n",
    "# Synthetic data\n",
    "x = torch.rand(sequence_length, batch_size, hidden_size).cuda().to(dtype=dtype)\n",
    "dy = torch.rand(sequence_length, batch_size, hidden_size).cuda().to(dtype=dtype)\n",
    "\n",
    "# Fp8 Configuration\n",
    "fp8_format = Format.HYBRID\n",
    "fp8_recipe = DelayedScaling(fp8_format=fp8_format, amax_history_len=16, amax_compute_algo=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the different parallelism strategies, let's first initialize NCCL process group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure parallel groups\n",
    "import os\n",
    "\n",
    "LOCAL_RANK = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "WORLD_SIZE = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n",
    "if not torch.distributed.is_initialized():\n",
    "    torch.distributed.init_process_group(\n",
    "        \"nccl\",\n",
    "        init_method=\"file:///tmp/rdzv\",\n",
    "        world_size=WORLD_SIZE,\n",
    "        rank=LOCAL_RANK,\n",
    "    )\n",
    "data_parallel_group = torch.distributed.new_group(ranks=[LOCAL_RANK], backend=\"nccl\")\n",
    "tensor_parallel_group = torch.distributed.new_group(ranks=[LOCAL_RANK], backend=\"nccl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributed code shown in this section needs to be executed with PyTorch's [torchrun (elastic launch)](https://pytorch.org/docs/stable/elastic/run.html) utility. Below is an example for launching a script on a single node with 4 GPUs:\n",
    "```bash\n",
    "$ torchrun --standalone --nnodes=1 --nprocs_per_node=4 script.py $SCRIPT_ARGS\n",
    "```\n",
    "\n",
    "The elastic launch utility spawns the requested number of processes on the specified number of nodes, and sets the `LOCAL_RANK` and `WORLD_SIZE` environment variables on every process that are required to initialize the process groups. For further guidance on running with multiple GPUs, please consult the documentation for [torch.distributed](https://pytorch.org/docs/stable/distributed.html).\n",
    "\n",
    "Note that Transformer Engine requires that each distributed process corresponds to exactly one GPU. In addition, we initialize the process groups for both data and tensor parallelism with the same GPUs to keep this example simple.\n",
    "\n",
    "In practice, there are multiple factors that can affect the optimal parallel layout: the system hardware, the network topology, usage of other parallelism schemes like pipeline parallelism. A rough rule-of-thumb is to interpret the GPUs as a 2D grid with dimensions of $\\text{num_nodes} \\times \\text{gpus_per_node}$. The rows are tensor-parallel groups and the columns are data-parallel groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parallelism\n",
    "\n",
    "The most common approach to parallel training distributes along the $\\text{batch_size}$ dimension. By storing duplicate copies of the model on each GPU, the forward and backward passes of the training step can be done independently, followed by a gradient synchronization.\n",
    "\n",
    "Enabling data parallelism with Transformer Engine is similar to enabling data parallelism with standard PyTorch models: simply wrap the modules with [torch.nn.parallel.DistributedDataParallel](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html). FP8 training requires extra synchronization for the scaling factors, so the data-parallel process group must also be passed to the [fp8_autocast](../api/pytorch.rst#transformer_engine.pytorch.fp8_autocast) context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 40.00059326171875 ms\n"
     ]
    }
   ],
   "source": [
    "import transformer_engine.pytorch as te\n",
    "import quickstart_utils as utils\n",
    "\n",
    "# Construct layer\n",
    "basic_transformer = te.TransformerLayer(\n",
    "    hidden_size,\n",
    "    ffn_hidden_size,\n",
    "    num_attention_heads,\n",
    "    params_dtype=dtype,\n",
    "    device='cuda',\n",
    ")\n",
    "data_parallel_transformer = torch.nn.parallel.DistributedDataParallel(\n",
    "    basic_transformer,\n",
    "    process_group=data_parallel_group,\n",
    ")\n",
    "\n",
    "# Training step\n",
    "with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe, fp8_group=data_parallel_group):\n",
    "    y = data_parallel_transformer(x)\n",
    "y.backward(dy)\n",
    "\n",
    "# Measure step time\n",
    "utils.speedometer(\n",
    "    data_parallel_transformer,\n",
    "    x,\n",
    "    dy,\n",
    "    forward_kwargs = { \"attention_mask\": None },\n",
    "    fp8_autocast_kwargs = {\n",
    "        \"enabled\": True,\n",
    "        \"fp8_recipe\": fp8_recipe,\n",
    "        \"fp8_group\": data_parallel_group,\n",
    "    },\n",
    ")\n",
    "\n",
    "del data_parallel_transformer, basic_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor and Sequence Parallelism\n",
    "\n",
    "A more advanced approach is to distribute the activation tensors along the $\\text{hidden_size}$ dimension. This allows us to scale past the limits of data parallelism (typically $\\text{hidden_size} > \\text{batch_size}$) and to reduce the per-GPU memory usage (since model parameters are also distributed), but it also incurs the overhead of communicating activation tensors between GPUs at every step. For a more detailed explanation, please see the [Megatron-LM paper](https://arxiv.org/pdf/1909.08053.pdf).\n",
    "\n",
    "In addition, sequence parallelism distributes along the $\\text{sequence_length}$ dimension. This can be used when tensor parallelism is enabled in order to parallelize operations that run outside the tensor-parallel region (e.g. layer norm). For more details, please see [this paper](https://arxiv.org/pdf/2205.05198.pdf).\n",
    "\n",
    "Transformer Engine modules have native support for tensor and sequence parallelism. If the user provides a process group for tensor parallelism, the modules will distribute the data and perform communication internally. If sequence parallelism is enabled, it will be applied for operations that are not amenable to tensor parallelism and it will use the tensor-parallel process group. In this case, the tensor parallel group must also be passed to the **fp8_group** argument in the [fp8_autocast](../api/pytorch.rst#transformer_engine.pytorch.fp8_autocast) context manager, either directly or as a subset of a larger distributed group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean time: 39.9941015625 ms\n"
     ]
    }
   ],
   "source": [
    "# Construct layer\n",
    "tensor_parallel_transformer = te.TransformerLayer(\n",
    "    hidden_size,\n",
    "    ffn_hidden_size,\n",
    "    num_attention_heads,\n",
    "    set_parallel_mode=True,\n",
    "    tp_group=tensor_parallel_group,\n",
    "    sequence_parallel=True,\n",
    "    params_dtype=dtype,\n",
    "    device='cuda',\n",
    ")\n",
    "data_tensor_parallel_transformer = torch.nn.parallel.DistributedDataParallel(\n",
    "    tensor_parallel_transformer,\n",
    "    process_group=data_parallel_group,\n",
    ")\n",
    "\n",
    "# Training step\n",
    "with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe, fp8_group=tensor_parallel_group):\n",
    "    y = data_tensor_parallel_transformer(x, attention_mask=None)\n",
    "y.backward(dy)\n",
    "\n",
    "# Measure step time\n",
    "utils.speedometer(\n",
    "    data_tensor_parallel_transformer,\n",
    "    x,\n",
    "    dy,\n",
    "    forward_kwargs = { \"attention_mask\": None },\n",
    "    fp8_autocast_kwargs = {\n",
    "        \"enabled\": True,\n",
    "        \"fp8_recipe\": fp8_recipe,\n",
    "        \"fp8_group\": tensor_parallel_group,\n",
    "    },\n",
    ")\n",
    "\n",
    "del data_tensor_parallel_transformer, tensor_parallel_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Sharded Data Parallel (FSDP)\n",
    "\n",
    "Transformer Engine is also supports PyTorch's [torch.distributed.fsdp.FullyShardedDataParallel](https://pytorch.org/docs/stable/fsdp.html) utility for parallelism.\n",
    "\n",
    "When FSDP wraps a PyTorch model, it combines all the module parameters in the model into a `FlatParameter` that is distributed over all the GPUs in the process group. Each FSDP-wrapped module only sees a view of this `FlatParameter` corresponding that module's local parameters on each GPU. Thus, the modules themselves execute forward and backward passes exclusively locally. Meanwhile, the FSDP wrapper is responsible for performing the all-gather before the forward and backward passes to ensure every module has the right weights, and the reduce-scatter after the backward pass to make sure the local gradients accumulates correctly.\n",
    "\n",
    "Using FSDP with Transformer Engine requires creating Transformer Engine modules without any inherent parallelism. For a Transformer layer, this means we only initialize a basic transformer without any process groups or sequence parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_tensor_parallel_transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Training step\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m te\u001b[38;5;241m.\u001b[39mfp8_autocast(enabled\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fp8_recipe\u001b[38;5;241m=\u001b[39mfp8_recipe, fp8_group\u001b[38;5;241m=\u001b[39mall_gpus):\n\u001b[0;32m---> 36\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mdata_tensor_parallel_transformer\u001b[49m(x, attention_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     37\u001b[0m y\u001b[38;5;241m.\u001b[39mbackward(dy)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Measure step time\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_tensor_parallel_transformer' is not defined"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel, MixedPrecision\n",
    "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "\n",
    "# Construct layer\n",
    "basic_transformer = te.TransformerLayer(\n",
    "    hidden_size,\n",
    "    ffn_hidden_size,\n",
    "    num_attention_heads,\n",
    "    params_dtype=dtype,\n",
    "    device='cuda',\n",
    ")\n",
    "\n",
    "# FSDP provides a wrapping policy tailored for Transformer architectures\n",
    "wrap_policy = partial(\n",
    "    transformer_auto_wrap_policy,\n",
    "    transformer_layer_cls={te.TransformerLayer},\n",
    ")\n",
    "\n",
    "# Generate new process group for FSDP and wrap the model\n",
    "all_gpus = torch.distributed.new_group(ranks=[LOCAL_RANK], backend=\"nccl\")\n",
    "fsdp_transformer = FullyShardedDataParallel(\n",
    "    basic_transformer,\n",
    "    process_group=all_gpus,\n",
    "    use_orig_params=True,\n",
    "    mixed_precision=MixedPrecision(\n",
    "        param_dtype=dtype,\n",
    "        reduce_dtype=torch.float32,\n",
    "    ),\n",
    "    sync_module_states=True,\n",
    "    auto_wrap_policy=wrap_policy\n",
    ")\n",
    "\n",
    "# Training step\n",
    "with te.fp8_autocast(enabled=True, fp8_recipe=fp8_recipe, fp8_group=all_gpus):\n",
    "    y = fsdp_transformer(x, attention_mask=None)\n",
    "y.backward(dy)\n",
    "\n",
    "# Measure step time\n",
    "utils.speedometer(\n",
    "    fsdp_transformer,\n",
    "    x,\n",
    "    dy,\n",
    "    forward_kwargs = { \"attention_mask\": None },\n",
    "    fp8_autocast_kwargs = {\n",
    "        \"enabled\": True,\n",
    "        \"fp8_recipe\": fp8_recipe,\n",
    "        \"fp8_group\": all_gpus,\n",
    "    },\n",
    ")\n",
    "\n",
    "del fsdp_transformer, basic_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing a basic transformer with no parallelism means the `te.TransformerLayer` is duplicated on every GPU same as the data parallel strategy. Since this is prohibitive to large models, TE supports deferred initialization via the `device='meta'` option. This initializes the layer with no memory allocation, and the model is materialized on device memory only after the FSDP wrap shards the parameters and recursively executes `model.reset_parameters()` on module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GPU-0] Memory use w/ CUDA init = 910.434304MiB\n",
      "[GPU-0] Memory use w/ meta init = 507.674624MiB\n",
      "[GPU-0] Memory use after FSDP = 910.434304MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n",
      "[rank0]:[W init.cpp:767] Warning: nvfuser is no longer supported in torch script, use _jit_set_nvfuser_enabled is deprecated and a no-op (function operator())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "810"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del x, y, dy\n",
    "gc.collect()\n",
    "\n",
    "# Construct basic transformer with 'cuda' device\n",
    "basic_transformer = te.TransformerLayer(\n",
    "    hidden_size,\n",
    "    ffn_hidden_size,\n",
    "    num_attention_heads,\n",
    "    params_dtype=dtype,\n",
    "    device='cuda',\n",
    ")\n",
    "mem_use = torch.cuda.memory_allocated(device=f\"cuda:{LOCAL_RANK}\") * 1e-6\n",
    "if LOCAL_RANK == 0:\n",
    "    print(f\"[GPU-{LOCAL_RANK}] Memory use w/ CUDA init = {mem_use}MiB\\n\", end='')\n",
    "del basic_transformer\n",
    "gc.collect()\n",
    "\n",
    "# Construct a basic transformer with 'meta' device\n",
    "basic_transformer_meta = te.TransformerLayer(\n",
    "    hidden_size,\n",
    "    ffn_hidden_size,\n",
    "    num_attention_heads,\n",
    "    params_dtype=dtype,\n",
    "    device='meta',\n",
    ")\n",
    "meta_mem_use = torch.cuda.memory_allocated(device=f\"cuda:{LOCAL_RANK}\") * 1e-6\n",
    "if LOCAL_RANK == 0:\n",
    "    print(f\"[GPU-{LOCAL_RANK}] Memory use w/ meta init = {meta_mem_use}MiB\\n\", end='')\n",
    "\n",
    "# Wrap with FSDP to materialize layer on device\n",
    "fsdp_transformer = FullyShardedDataParallel(\n",
    "    basic_transformer_meta,\n",
    "    process_group=all_gpus,\n",
    "    use_orig_params=True,\n",
    "    mixed_precision=MixedPrecision(\n",
    "        param_dtype=dtype,\n",
    "        reduce_dtype=torch.float32,\n",
    "    ),\n",
    "    sync_module_states=True,\n",
    "    auto_wrap_policy=wrap_policy\n",
    ")\n",
    "fsdp_mem_use = torch.cuda.memory_allocated(device=f\"cuda:{LOCAL_RANK}\") * 1e-6\n",
    "if LOCAL_RANK == 0:\n",
    "    print(f\"[GPU-{LOCAL_RANK}] Memory use after FSDP = {fsdp_mem_use}MiB\\n\", end='')\n",
    "del fsdp_transformer, basic_transformer_meta\n",
    "gc.collect()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
